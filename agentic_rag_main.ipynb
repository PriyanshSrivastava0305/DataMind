{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxBjerPVqjmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834e2ab9-7121-4588-b0ca-7b60b161e7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pypdf langchain_community langchain_huggingface langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZapUF_aFx8-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e97fe9-0c36-44c4-ef24-7a3d5fdc3539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "··········\n",
            "Upload your PDF file path: /content/hdr_2020_overview_english.pdf\n",
            "Loaded 36 document chunks.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_groq import ChatGroq\n",
        "import requests  # For SerpApi requests\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "\n",
        "file_path = input(\"Upload your PDF file path: \")\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} document chunks.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krVFk-nfx_Kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6a6aa2-33bf-4dde-da19-7860e66af33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "texts = [split.page_content for split in splits]\n",
        "vectors = embedding_model.encode(texts, convert_to_tensor=False)\n",
        "\n",
        "# Vector Store\n",
        "vectorstore = InMemoryVectorStore.from_documents(\n",
        "    documents=splits, embedding=HuggingFaceEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSEhUbrlyCG-"
      },
      "outputs": [],
      "source": [
        "def search_web(query):\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"api_key\": \"ee88a9dae8ac4815cb320fe1712a85a48df5cf2c035ff848b2d5f137fafe87bd\",\n",
        "        \"num\": 2,\n",
        "    }\n",
        "    response = requests.get(\"https://serpapi.com/search.json\", params=params)\n",
        "    results = response.json()\n",
        "    web_content = []\n",
        "\n",
        "    # Parse results to retrieve snippets and URLs\n",
        "    for result in results.get(\"organic_results\", []):\n",
        "        snippet = result.get(\"snippet\", \"\")\n",
        "        link = result.get(\"link\", \"\")\n",
        "        web_content.append(f\"{snippet} (Source: {link})\")\n",
        "\n",
        "    return \"\\n\".join(web_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ctu6enUyEcg"
      },
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following retrieved PDF content and web search results to answer the question. \"\n",
        "    \"If you don't know the answer, say that you don't know. \"\n",
        "    \"Use concise language.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L7qTBxMyG1H"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts.chat import SystemMessage, HumanMessage\n",
        "\n",
        "def query_agent(query):\n",
        "    pdf_results = rag_chain.invoke({\"input\": query})\n",
        "    web_results = search_web(query)\n",
        "\n",
        "    combined_context = f\"PDF Context:\\n{pdf_results}\\n\\nWeb Search Context:\\n{web_results}\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are an assistant for answering questions based on PDF and web data.\"),\n",
        "        SystemMessage(content=combined_context),\n",
        "        HumanMessage(content=query),\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "    answer_content = response.get(\"content\", \"I'm here to help with more questions if needed.\")\n",
        "    return answer_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xic5slONyImS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be0439ea-e8f6-40e6-984e-77b9996db87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'exit' to stop): hi\n",
            "\n",
            "Answer: Hi! I'm here to help answer your questions. What would you like to know?\n",
            "\n",
            "You can ask me anything, and I'll do my best to provide you with accurate information. You can ask me questions about the PDF document or web search context that we have. What's on your mind? \n",
            "\n",
            "Enter your question (type 'exit' to stop): what is the document about \n",
            "\n",
            "Answer: Based on the provided PDF context, the document appears to be the Human Development Report. The report provides an overview of human development and progress towards achieving the Sustainable Development Goals (SDGs). \n",
            "\n",
            "Enter your question (type 'exit' to stop): who are you\n",
            "\n",
            "Answer: I am an assistant designed to help answer questions based on the content of a provided PDF and web search results. I can provide concise and accurate answers to your questions, but if I'm unsure or don't have enough information, I will say \"I don't know.\" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def query_agent(query):\n",
        "    pdf_results = rag_chain.invoke({\"input\": query})\n",
        "    web_results = search_web(query)\n",
        "\n",
        "    combined_context = f\"PDF Context:\\n{pdf_results}\\n\\nWeb Search Context:\\n{web_results}\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are an assistant for answering questions based on PDF and web data.\"),\n",
        "        SystemMessage(content=combined_context),\n",
        "        HumanMessage(content=query),\n",
        "    ]\n",
        "\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    answer_content = response.content if hasattr(response, \"content\") else \"Sorry, I couldn't find an answer.\"\n",
        "    return answer_content\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"Enter your question (type 'exit' to stop): \")\n",
        "    if user_query.lower() == \"exit\":\n",
        "        print(\"Exiting the chat. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = query_agent(user_query)\n",
        "    print(\"\\nAnswer:\", response, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tC1b3uNZK53_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}